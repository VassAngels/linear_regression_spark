{"cells":[{"cell_type":"markdown","source":["#### Project Description - Predict Power Emission using Linear Regression\n\n0. Load in the dataset from UCI ML Repository https://archive.ics.uci.edu/ml/index.php. \n0. Determine and evaluate a Baseline model\n0. Build and evaluate a Linear Regression Model using SparkML"],"metadata":{}},{"cell_type":"markdown","source":["###1. Load Data\n\nImport https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant\n\n\nData Set Information:\n\nThe dataset contains 9568 data points collected from a Combined Cycle Power Plant over 6 years (2006-2011), when the power plant was set to work with full load. Features consist of hourly average ambient variables Temperature (AT), Ambient Pressure (AP), Relative Humidity (RH) and Exhaust Vacuum (V) to predict the net hourly electrical energy output (PE) of the plant.\nA combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is colected from and has effect on the Steam Turbine, he other three of the ambient variables effect the GT performance.\nFor comparability with our baseline studies, and to allow 5x2 fold statistical tests be carried out, we provide the data shuffled five times. For each shuffling 2-fold CV is carried out and the resulting 10 measurements are used for statistical testing.\nWe provide the data both in .ods and in .xlsx formats.\n\n\nAttribute Information:\n\nFeatures consist of hourly average ambient variables \n- Temperature (AT) in the range 1.81°C and 37.11°C,\n- Ambient Pressure (AP) in the range 992.89-1033.30 milibar,\n- Relative Humidity (RH) in the range 25.56% to 100.16%\n- Exhaust Vacuum (V) in teh range 25.36-81.56 cm Hg\n- Net hourly electrical energy output (PE) 420.26-495.76 MW\nThe averages are taken from various sensors located around the plant that record the ambient variables every second. The variables are given without normalization."],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/powerplant.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \";\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location) \\\n  .na.drop()\n\ndisplay(df.take(2))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>time</th><th>AT</th><th>V</th><th>AP</th><th>RH</th><th>PE</th></tr></thead><tbody><tr><td>2017-01-01T00:00:00.000+0000</td><td>14.96</td><td>41.76</td><td>1024.07</td><td>73.17</td><td>463.26</td></tr><tr><td>2017-01-01T01:00:00.000+0000</td><td>25.18</td><td>62.96</td><td>1020.04</td><td>59.08</td><td>444.37</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["#####Examine distribution of target variable 'PE'"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n# display(df.select('PE'))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nprint(df.select(mean('PE')).show())\nmedian=df.approxQuantile('PE',[0.5],0)\n# print(f'Median: {median}')\nprint('Median: %f' %median[0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+\n          avg(PE)|\n+-----------------+\n454.3598890168576|\n+-----------------+\n\nNone\nMedian: 451.580000\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["####2. Baseline Model"],"metadata":{}},{"cell_type":"code","source":["#Import dependencies\nfrom pyspark.sql.functions import *\n\n#Adding Average \nbaseline = df.withColumn(\"averagePE\", lit(454.0))\n\n#Imputing Null Values\nbaseline = baseline.na.fill(454.0,'PE')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nrmseRegressionEvaluator = RegressionEvaluator(labelCol='PE', metricName ='rmse', predictionCol='averagePE')\nrmse=rmseRegressionEvaluator.evaluate(baseline)\n# print(f'RMSE is: {rmse}')\nr2RegressionEvaluator =RegressionEvaluator(labelCol='PE', metricName ='r2', predictionCol='averagePE')\nr2=r2RegressionEvaluator.evaluate(baseline)\n# print(f'R squared: {r2}')\n\n# print(listItems)  \nhtml = \"\"\"\n<body>\n  <h2>Baseline Performance Metrics - RMSE and R2</h2>\n  %s\n</body>\n\"\"\" % (f'RMSE: {rmse}     R2: {r2}')\n\ndisplayHTML(html)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n<body>\n  <h2>Baseline Performance Metrics - RMSE and R2</h2>\n  RMSE: 17.06789411765725     R2: -0.00044480579813832577\n</body>\n"]}}],"execution_count":9},{"cell_type":"markdown","source":["###3. Linear Regression Model"],"metadata":{}},{"cell_type":"markdown","source":["#####Splitting Data into Train and Test"],"metadata":{}},{"cell_type":"code","source":["(trainDF, testDF) = df.randomSplit([.8, .2], seed=42)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["#####Build and train model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\n#Vectorising features columns in preparation for feeding into Linear Regression\n#Define Vector Assembler\nvecAssembler = VectorAssembler(inputCols = [\"AT\", \"V\", \"AP\", \"RH\"], outputCol = \"features\")\n\n#Run train/test data through the assembler\nvecTrainDF = vecAssembler.transform(trainDF)\nvecTestDF = vecAssembler.transform(testDF)\n\n#Create Linear Regression Model\nlr = LinearRegression(featuresCol = \"features\", labelCol = \"PE\")\n\n# Train Model\nlrModel = lr.fit(vecTrainDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["#####Make predictions using the test data"],"metadata":{}},{"cell_type":"code","source":["predDF=lrModel.transform(vecTestDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["display(predDF.take(2))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>time</th><th>AT</th><th>V</th><th>AP</th><th>RH</th><th>PE</th><th>features</th><th>prediction</th></tr></thead><tbody><tr><td>2017-01-01T01:00:00.000+0000</td><td>25.18</td><td>62.96</td><td>1020.04</td><td>59.08</td><td>444.37</td><td>List(1, 4, List(), List(25.18, 62.96, 1020.04, 59.08))</td><td>444.11690697929447</td></tr><tr><td>2017-01-01T02:00:00.000+0000</td><td>5.11</td><td>39.4</td><td>1012.16</td><td>92.14</td><td>488.56</td><td>List(1, 4, List(), List(5.11, 39.4, 1012.16, 92.14))</td><td>483.5504389801087</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["#####Evaluate the model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nrmseRegressionEvaluator = RegressionEvaluator(labelCol='PE', metricName ='rmse', predictionCol='prediction')\nrmse=rmseRegressionEvaluator.evaluate(predDF)\nprint({rmse})\nr2RegressionEvaluator =RegressionEvaluator(labelCol='PE', metricName ='r2', predictionCol='prediction')\nr2=r2RegressionEvaluator.evaluate(predDF)\nprint({r2})\n\n\n\n# print(listItems)  \nhtml = \"\"\"\n<body>\n  <h2>Linear Regression's Performance Metrics - RMSE and R2</h2>\n  %s\n</body>\n\"\"\" % (f'RMSE: {rmse}     R2: {r2}')\n\ndisplayHTML(html)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n<body>\n  <h2>Linear Regression's Performance Metrics - RMSE and R2</h2>\n  RMSE: 4.583897476391786     R2: 0.9290129741098317\n</body>\n"]}}],"execution_count":19},{"cell_type":"markdown","source":["####By comparing the performance metrics we can see the Linear Regression model performs way better than using the Baseline to predict PE as expected"],"metadata":{}}],"metadata":{"name":"LinearRegressionExample","notebookId":1433009670887740},"nbformat":4,"nbformat_minor":0}
